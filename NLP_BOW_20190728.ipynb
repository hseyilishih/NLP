{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red>NLP Bag of Words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:yellow>1. import 所需套件 </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:grey>tfidf</mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csr import csr_matrix #if you want to save tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:grey>formatting 所需套件 </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese (Traditional)_Taiwan.950'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:grey>視覺化 所需套件 </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:yellow> 練習 sklearn.feature_extraction.text --- CountVectorizer </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myvectorizer_instance = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark style=background-color:grey> open(your_txt).read() </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = open('C:/Users/USER/NLP_JERRYWU/data/carl_sagan_quote4.txt').read()\n",
    "#We've arranged a civilization in which most crucial elements profoundly depend on science and technology.\n",
    "\n",
    "text2 = open('C:/Users/USER/NLP_JERRYWU/data/carl_sagan_quote2.txt').read()\n",
    "#We have also arranged things so that almost no one understands science and technology. This is a prescription for disaster. We might get away with it for a while, but sooner or later this combustible mixture of ignorance and power is going to blow up in our faces.\n",
    "\n",
    "text3 = open('C:/Users/USER/NLP_JERRYWU/data/carl_sagan_quote3.txt').read()\n",
    "#Every one of us is, in the cosmic perspective, precious. If a human disagrees with you, let him live. In a hundred billion galaxies, you will not find another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = [text1,text2,text3] #3段文字元素 打包在一個 LIST 引號+逗點分開\n",
    "my_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark style=background-color:grey> 將文本丟入    訓練 vectorizer.fit_transform(my_corpus) </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_fit_transform = myvectorizer_instance.fit_transform(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出 feature name, 每個name transpose為 row, column 則為原來文本 包在list 的順序 text1/2/3\n",
    "df = pd.DataFrame(vectorizer_fit_transform.toarray().transpose(), index=myvectorizer_instance.get_feature_names())\n",
    "df #此時column name 是類似 index 0 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['CS_text1','CS_text2','CS_text3'] #幫每個 column 命名 \n",
    "df\n",
    "#彙總所有text1+2+3的字 當作row , 顯示每一個字出現在每一篇文本段的次數  \n",
    "#可能某個字出現在每一篇,或者某篇出現特別多次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark style=background-color:YELLOW> 新的練習題 Bag of words 字袋</mark> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:grey> jieba.cut</mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open(\"C:/Users/USER/NLP_JERRYWU/data/jay.txt\",encoding='utf8').read() \n",
    "'''\n",
    "你住的巷子裡 我租了一間公寓 爲了想與你不期而遇 高中三年 我爲什麽 爲什麽不好好讀書 沒考上跟你一樣的大學 我找了份工作 離你宿舍很近 當我開始學會做蛋餅 才發現你不吃早餐\n",
    "喔 你又擦肩而過 你耳機聽什麽 能不能告訴我 躺在你學校的操場看星空 教室裡的燈 還亮著你沒走 記得我寫給你的情書 都什麽年代了 到現在我還在寫著 總有一天 總有一年 會發現 有人默默的陪在你的身邊 也許我不該在你的世界 當你收到情書 也代表我已經走遠  (楊) 更多更詳盡歌詞 在 ※ Mojim.com　魔鏡歌詞網  學校旁的廣場 我在這等鐘聲響 等你下課一起走好嗎 (合) 彈著琴 唱你愛的歌 暗戀一點都不痛苦 痛苦的是你根本沒看我  我唱這麽走心 卻走不進你心裡 在人來人往找尋著你 守護著你不求結局 喔 你又擦肩而過 我唱告白氣球 終於你回了頭  躺在你學校的操場看星空 教室裡的燈 還亮著你沒走 記得我寫給你的情書 都什麽年代了 到現在我還在寫著  總有一天 總有一年 會發現 有人默默的陪在你的身邊 也許我不該在你的世界 當你收到情書 也代表我已經走遠\n",
    "'''\n",
    "corpus = list(jieba.cut(raw, cut_all=True))\n",
    "print(len(corpus)) #將會轉成 481 column\n",
    "print(*corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark style=background-color:grey> 訓練 vectorizer.fit_transform(my_corpus) </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec = myvectorizer_instance.fit_transform(corpus)\n",
    "doc_vec    # 訓練後得到 35 feature rows  (文本 被 jieba cut成 481 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index=myvectorizer_instance.get_feature_names())\n",
    "#df #0~480   35 rows × 481 columns ; row 是 vectorizer的fieature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(df) #盤點出所有column name\n",
    "#col_list.remove('xxx') #去除掉不要的column name\n",
    "\n",
    "df['Col_total'] = df[col_list].sum(axis=1) #column wise 從左邊加到右邊\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將該詞出現的次數加總 成為一個獨立column Row_total\n",
    "df.loc['Row_total'] = df.select_dtypes(pd.np.number).sum()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_feature'] = df.index\n",
    "\n",
    "#本來 name_feature是index, 將其rename \n",
    "\n",
    "#df.reset_index(level=0, inplace=True)\n",
    "#df.rename(columns={\"index\": \"name_feature\"}, inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先用total sort descending 再用name_feature sort ascending\n",
    "df.sort_values(by=['Col_total', 'name_feature'], ascending=[False,True], inplace=True)\n",
    "#, na_position='first'  , if you want put NAs first\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_duplicate = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你不希望 最後一個total row也被納入排序,可以先將dataframe 拆成兩份 然後再併回來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = df[df['name_feature'] == 'Row_total']\n",
    "#excluded\n",
    "included = df[df['name_feature'] != 'Row_total'] \n",
    "#included\n",
    "included.sort_values(by=['Col_total', 'name_feature'], ascending=[False,True], inplace=True)\n",
    "df_new = included.append(excluded)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "#文本轉換詞頻矩陣\n",
    "vectorizer = CountVectorizer()\n",
    "#計算詞語出現次數\n",
    "X = vectorizer.fit_transform(corpus) \n",
    "#文本關鍵字獲取\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "print('type of feature_name', type(feature_name), '; length=', len(feature_name))\n",
    "print(feature_name)\n",
    "\n",
    "#轉成 arrary 0 1值 檢查結果\n",
    "print(X.toarray())\n",
    "print(type(X.toarray()))\n",
    "\n",
    "#第一段字This is the first document.  有出現 document, first, is, the, this 沒錯\n",
    "# 大小寫字視為相同\n",
    "# 印出 array時 沒有看到像list般的 逗點區隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red> TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "#文本轉換詞頻矩陣\n",
    "vectorizer = CountVectorizer()\n",
    "#計算詞語出現次數\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print('type(X)=', type(X),' ; X.shape=', X.shape,  '\\n')\n",
    " \n",
    "#文本關鍵字獲取\n",
    "word = vectorizer.get_feature_names()\n",
    "print('type(get_feature_names)=', type(word), '; length=', len(word),'\\n', 'feature_names=', word,'\\n')\n",
    "#檢查結果\n",
    "print('X.toarray()=','\\n',X.toarray(),'\\n')\n",
    " \n",
    "#功能調用\n",
    "transformer = TfidfTransformer()\n",
    "print(transformer)\n",
    "\n",
    "#將 X轉成TF-IDF值\n",
    "\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print (tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    " \n",
    "tf = TfidfVectorizer()\n",
    "tfidf_matrix =  tf.fit_transform(corpus)\n",
    "print('type(tfidf_matrix)=', type(tfidf_matrix), '\\n')\n",
    "\n",
    "feature_names = tf.get_feature_names()\n",
    "print('type(feature_names)=', type(feature_names), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_name)\n",
    "for x in tfidf_matrix:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "\n",
    "for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "    print (w,'\\t', s)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark style=background-color:grey>TF-IDF中文 </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    '你 住 的 巷子 裡',\n",
    "    '我 租了 一間 公寓',\n",
    "    '爲了 想 與 你 不期而遇',\n",
    "    '高中 三年',\n",
    "]\n",
    "#文本轉換詞頻矩陣\n",
    "vectorizer = CountVectorizer()\n",
    "#計算詞語出現次數\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#文本關鍵字獲取\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "\n",
    "#檢查結果\n",
    "print(X.toarray())\n",
    " \n",
    "#功能調用\n",
    "transformer = TfidfTransformer()\n",
    "print(transformer)\n",
    "#將矩陣X轉成TF-IDF值\n",
    "tfidf = transformer.fit_transform(X)\n",
    "\n",
    "\n",
    "print (tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各feature_names tf-idf權重***** \n",
      "\n",
      "\n",
      " *****第 1 段文= 你 住 的 巷子 裡\n",
      "巷子 1.0\n",
      "\n",
      " *****第 2 段文= 我 租了 一間 公寓\n",
      "一間 0.5773502691896257\n",
      "公寓 0.5773502691896257\n",
      "租了 0.5773502691896257\n",
      "\n",
      " *****第 3 段文= 爲了 想 與 你 不期而遇\n",
      "不期而遇 0.7071067811865476\n",
      "爲了 0.7071067811865476\n",
      "\n",
      " *****第 4 段文= 高中 三年\n",
      "三年 0.7071067811865476\n",
      "高中 0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "__author__ = \"XXX\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus=[\"你 住 的 巷子 裡\",\n",
    "\t\t\"我 租了 一間 公寓\",\n",
    "\t\t\"爲了 想 與 你 不期而遇\",\n",
    "\t\t\"高中 三年\"]\n",
    "    vectorizer=CountVectorizer()\n",
    "    transformer=TfidfTransformer()\n",
    "    tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "    #左邊第一個fit_transform計算 tf-idf，右邊第二個 fit_transform 是文本轉為詞頻矩陣\n",
    "    word=vectorizer.get_feature_names() #獲取詞袋模型中所有詞語feature_names\n",
    "    weight=tfidf.toarray() #抽取TF-IDF矩陣，元素a[i][j]表示 j詞 在第 i篇文本中的 tf-idf權重\n",
    "    print (\"各feature_names tf-idf權重*****\",'\\n')\n",
    "    \n",
    "    for i in range(len(weight)):\n",
    "        print(\"\\n *****第\",i+1,\"段文=\", corpus[i])\n",
    "\n",
    "        for j in range(len(word)):\n",
    "            if weight[i][j] >0:\n",
    "                print (word[j],weight[i][j])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
